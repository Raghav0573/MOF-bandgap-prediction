{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ChQAhC2A1QR"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import requests\n",
        "from io import StringIO\n",
        "warnings.filterwarnings('ignore')\n",
        "import gc\n",
        "import joblib\n",
        "\n",
        "!pip install dscribe\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "\n",
        "from dscribe.descriptors import ACSF\n",
        "from ase import Atoms\n",
        "from ase.io import read"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION CELL ---\n",
        "# DO NOT CHANGE THE VARIABLE NAMES BELOW\n",
        "\n",
        "# Path to your saved model (relative to this notebook)\n",
        "MODEL_PATH = \"model.pt\"\n",
        "\n",
        "# Name of the test CSV file\n",
        "# Note: For classification/regression tasks, this file has the same columns\n",
        "# as train.csv, minus the target column.\n",
        "INPUT_CSV_NAME = \"test.csv\"\n",
        "\n",
        "# Folder containing external files (Only applicable for Domain A: Inorganic)\n",
        "# For A1/A2, this folder contains the test.csv and all corresponding .cif files.\n",
        "INPUT_FOLDER_NAME = \"test_dataset\"\n"
      ],
      "metadata": {
        "id": "8-r_KRjWDTXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the master CSV file with file name and energy\n",
        "master_df = pd.read_csv(INPUT_CSV_NAME)\n",
        "master_df.columns = ['cif_files','formula','pld','lcd','density','energy']\n",
        "len(master_df)"
      ],
      "metadata": {
        "id": "hFWNnDBPEELG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "master_df[\"ID_num\"] = master_df[\"cif_files\"].str.extract(r\"MOF_(\\d+)\").astype(int)\n",
        "master_df = master_df.sort_values(by=\"ID_num\").reset_index(drop=True)\n",
        "\n",
        "# Defining features (X) and target (y) based on your plots\n",
        "features = ['energy', 'pld', 'lcd', 'density']\n",
        "\n",
        "X_global = master_df[features]\n",
        "master_df.head()"
      ],
      "metadata": {
        "id": "UGdiMEbBEX_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a global atom list\n",
        "def get_global_species(cif_files):\n",
        "    species = set()\n",
        "\n",
        "    for file in cif_files:\n",
        "        try:\n",
        "            atoms = read(f'{INPUT_FOLDER_NAME}/{file}.cif')\n",
        "            species.update(atoms.get_chemical_symbols())\n",
        "        except Exception:\n",
        "            print(f\"Failed to read CIF file: {file}.cif\")\n",
        "\n",
        "    return sorted(species)\n",
        "\n",
        "# Call the function to get the total list\n",
        "atom_set = get_global_species(master_df['cif_files'])"
      ],
      "metadata": {
        "id": "-0GnvaEGEeCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build and use the symmetry function**"
      ],
      "metadata": {
        "id": "mAW4sfflEi3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate the acsf parameters\n",
        "def generate_acsf(cif_file, acsf_params):\n",
        "\n",
        "    try:\n",
        "        atoms = read(f'{cif_file}')  # Read structure directly from CIF file using ASE\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to read CIF file: {cif_file}\")\n",
        "        return None\n",
        "\n",
        "    # Initialize ACSF descriptor\n",
        "    acsf = ACSF(**acsf_params)  # Create an ACSF descriptor object with the specified parameters.\n",
        "\n",
        "    # Generate ACSF descriptors (n_atoms x n_features)\n",
        "    acsf_descriptors = acsf.create(atoms)  # Generate ACSF descriptors for the molecule.\n",
        "    return acsf_descriptors  # Return the ACSF descriptors.\n",
        "\n",
        "# Function to generate the acsf parameters\n",
        "def create_streamed(df, acsf_params, n_features, out_path=\"/content/acsf_descriptors.dat\", size_aware=True):\n",
        "\n",
        "    n_samples = len(df)\n",
        "\n",
        "    output = np.memmap(\n",
        "        out_path,\n",
        "        dtype=\"float32\",\n",
        "        mode=\"w+\",\n",
        "        shape=(n_samples, n_features)\n",
        "    )\n",
        "\n",
        "    # Create ACSF\n",
        "    acsf = ACSF(**acsf_params)\n",
        "\n",
        "    for i, cif_file in enumerate(df[\"cif_files\"]):\n",
        "        try:\n",
        "            atoms = read(f\"{INPUT_FOLDER_NAME}/{cif_file}.cif\")\n",
        "            desc = acsf.create(atoms)\n",
        "        except Exception:\n",
        "            output[i] = np.zeros(n_features, dtype=np.float32)\n",
        "            continue\n",
        "\n",
        "        if desc.shape[0] == 0:\n",
        "            output[i] = np.zeros(n_features, dtype=np.float32)\n",
        "        else:\n",
        "            if size_aware:\n",
        "                output[i] = desc.sum(axis=0) / desc.shape[0]\n",
        "            else:\n",
        "                output[i] = desc.mean(axis=0)\n",
        "\n",
        "        # Clean up\n",
        "        del atoms, desc\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "qexNAIdDEnK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ACSF parameters\n",
        "acsf_params = {  # Define ACSF parameters for descriptor generation.\n",
        "        \"species\": atom_set,  # List of atomic species to consider.\n",
        "        \"r_cut\": 5.0,  # Cutoff radius for ACSF calculations.\n",
        "        \"g2_params\": [[1, 1], [1, 3]],  # Parameters for G2 symmetry functions.\n",
        "        \"g4_params\": [[1, 1, 1]],  # Parameters for G4 symmetry functions.\n",
        "    }\n",
        "\n",
        "# Get n_features to set the feature vector size\n",
        "sample_desc = generate_acsf(f\"{INPUT_FOLDER_NAME}/{master_df['cif_files'].iloc[0]}.cif\", acsf_params)\n",
        "n_features = sample_desc.shape[1]\n",
        "del sample_desc\n",
        "\n",
        "# Generate X_values for the model\n",
        "X = create_streamed(master_df, acsf_params, n_features)\n",
        "# Get y values\n",
        "# y = master_df['band_gap'].to_numpy()\n",
        "# y = y.reshape(-1,1)\n",
        "\n",
        "# Y-scaled\n",
        "# scaler_y = StandardScaler()\n",
        "# y_scaled = scaler_y.fit_transform(y)\n",
        "# joblib.dump(scaler_y, \"y_scaler.pkl\") # Save for future use\n",
        "\n",
        "# Scale the input values in batches\n",
        "scaler = joblib.load(\"X_acsf_scaler.pkl\")\n",
        "batch_size = 50\n",
        "\n",
        "for i in range(0, X.shape[0], batch_size):\n",
        "    scaler.transform(X[i:i+batch_size])\n",
        "\n",
        "X_scaled = np.memmap(\"/content/acsf_scaled.dat\", dtype=\"float32\", mode=\"w+\", shape=X.shape)\n",
        "\n",
        "for i in range(0, X.shape[0], batch_size):\n",
        "    X_scaled[i:i+batch_size] = scaler.transform(X[i:i+batch_size])"
      ],
      "metadata": {
        "id": "7cy4yKK3ExSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_glob_scaler = joblib.load(\"X_global_scaler.pkl\")\n",
        "X_glob_scaled = X_glob_scaler.transform(X_global)\n",
        "\n",
        "class IOChemInferenceDataset(Dataset):\n",
        "    def __init__(self, X_memmap, X_global, indices=None):\n",
        "        self.X = X_memmap\n",
        "        self.Xg = X_global\n",
        "        self.indices = indices if indices is not None else np.arange(len(X_memmap))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.indices[i]\n",
        "        x = torch.from_numpy(self.X[idx]).float()\n",
        "        xg = torch.from_numpy(self.Xg[idx]).float()\n",
        "        return x, xg\n",
        "\n",
        "infer_dataset = IOChemInferenceDataset(\n",
        "    X_scaled,\n",
        "    X_glob_scaled\n",
        ")\n",
        "\n",
        "infer_loader = DataLoader(\n",
        "    infer_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "dOgO1THiKSUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define the Autoencoder**"
      ],
      "metadata": {
        "id": "LeiLVWo3IRqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model architecture\n",
        "LATENT_DIM = 384\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 200\n",
        "LEARNING_RATE = 0.001\n",
        "ALPHA = 0.4  # Weight for reconstruction loss\n",
        "BETA = 0.6   # Weight for property prediction loss\n",
        "PATIENCE = 10  # Early stopping patience\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "K6yjn2xEIW-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Autoencoder Model\n",
        "class MolecularAutoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoencoder for molecular property prediction\n",
        "\n",
        "    Architecture:\n",
        "    - Encoder: Compresses molecular fingerprints to latent representation\n",
        "    - Decoder: Reconstructs fingerprints from latent space\n",
        "    - Property Predictor: Predicts logP and logS from latent representation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, latent_dim=384, num_properties=1):\n",
        "        super(MolecularAutoencoder, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_properties = num_properties\n",
        "\n",
        "        # Encoder Network\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(512, latent_dim),\n",
        "            nn.BatchNorm1d(latent_dim),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder Network\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(1024, input_dim),\n",
        "        )\n",
        "\n",
        "        # Property Prediction Network\n",
        "        self.property_predictor = nn.Sequential(\n",
        "            nn.Linear(latent_dim+4, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(64, num_properties)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Encode input to latent representation\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"Decode latent representation to reconstruction\"\"\"\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x, xg):\n",
        "        \"\"\"\n",
        "        Forward pass through autoencoder\n",
        "\n",
        "        Returns:\n",
        "            reconstructed: Reconstructed fingerprints\n",
        "            predicted_properties: Predicted molecular properties\n",
        "            latent: Latent representation\n",
        "        \"\"\"\n",
        "        latent = self.encode(x)\n",
        "        reconstructed = self.decode(latent)\n",
        "        combined = torch.cat([latent, xg], dim=1)\n",
        "        predicted_properties = self.property_predictor(combined)\n",
        "\n",
        "        return reconstructed, predicted_properties, latent\n",
        "\n",
        "print(\"Autoencoder model class defined\")"
      ],
      "metadata": {
        "id": "IYBz1J4sIZJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "input_dim = X.shape[1]\n",
        "num_properties = 1\n",
        "\n",
        "model = MolecularAutoencoder(\n",
        "    input_dim=input_dim,\n",
        "    latent_dim=LATENT_DIM,\n",
        "    num_properties=num_properties\n",
        ").to(DEVICE)\n",
        "\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "print(\"Best model loaded for evaluation\")\n",
        "model.eval()\n",
        "predictions = []\n",
        "row_numbers = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    row_count = 0\n",
        "    for x, xg in infer_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        xg = xg.to(DEVICE)\n",
        "\n",
        "        _, preds, _ = model(x, xg)   # autoencoder output\n",
        "        preds = preds.squeeze().cpu().numpy()\n",
        "\n",
        "        batch_size = len(preds)\n",
        "        predictions.extend(preds.tolist())\n",
        "        row_numbers.extend(range(row_count, row_count + batch_size))\n",
        "        row_count += batch_size"
      ],
      "metadata": {
        "id": "uwR1ELhlLfxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Saving the predictions**"
      ],
      "metadata": {
        "id": "_y-kYfsCSQmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_scaler = joblib.load(\"y_scaler.pkl\")\n",
        "predictions = y_scaler.inverse_transform(\n",
        "    np.array(predictions).reshape(-1, 1)\n",
        ").flatten()\n",
        "\n",
        "df_preds = pd.DataFrame({\n",
        "    \"Row Number\": row_numbers,\n",
        "    \"Predicted Value\": predictions\n",
        "})\n",
        "\n",
        "df_preds.to_csv(\"10_inference_predictions_A1.csv\", index=False)"
      ],
      "metadata": {
        "id": "OR9FAnIrSUGu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}